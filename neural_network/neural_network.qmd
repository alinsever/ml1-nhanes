# Neural Network on daily Alcohol Consumption

A previous document explored whether their a link between the numbers of drinks a day and physical and mental health.
Focusing on statistical inference, the Poisson GAM model showed multiple statistical effects but also demonstrated very weak predictive capabilities.
This document expends this document by trying to build a multi-layered perceptron neural network that accurately predicts daily alcohol consumption.

```{r}
library(gamlss.add)
library(nnet)
library(caret)
library(ggplot2)
library(readr)
library(here)
library(dplyr)
library(plotly)
library(reticulate)
library(torch)
library(luz)
```

```{r}
nhanes <- readRDS(here("nhanes.rds"))

nhanes <- nhanes %>%
				mutate(
							 Gender = as.factor(Gender),
							 AgeDecade = as.factor(AgeDecade),
							 Race1 = as.factor(Race1),
							 Education = as.factor(Education),
							 MaritalStatus = as.factor(MaritalStatus),
							 HHIncome = as.factor(HHIncome),
							 HealthGen = as.factor(HealthGen),
							 PhysActive = as.factor(PhysActive),
							 SleepTrouble = as.factor(SleepTrouble),
							 Alcohol12PlusYr = as.factor(Alcohol12PlusYr),
							 SmokeNow = as.factor(SmokeNow),
							 Smoke100 = as.factor(Smoke100),
							 LittleInterest = as.factor(LittleInterest),
							 Depressed = as.factor(Depressed)
				)
```

# Data Cleaning

## Missing Values

The more data we have at our disposal, the better. 
During the previous GLM - Poisson analysis, we cleaned the data in such as way that only 2318 observation were left.
Although sufficient for statistical inference, it is not enough to maximize performance of a neural network.
By only using the significant predictor found and removing rows containing missing values, we are able to gather 4435 observations.
This achieve two major goals, limiting dimensionality issue due to the inclusion of two many features, as well as maximizing information content inside the training data by only keeping important feature, and be sure to have as much complete observation as possible by reducing the amount of features will use.

```{r}
best_pred_nhanes <- nhanes %>% 
				dplyr::select(AlcoholDay, Education, Depressed, Gender, Age, Poverty, SleepHrsNight, BPSysAve) %>%
				tidyr::drop_na()


prev_nhanes <- nhanes %>%
				dplyr::select(AlcoholDay, Weight, BPSysAve, PhysActiveDays, SleepHrsNight, Depressed, DaysMentHlthBad, Education, Poverty, Gender, SurveyYr, Age) %>%
				tidyr::drop_na()

print(paste0("N. observation using GLM - Poisson method of missing values cleaning: ", dim(prev_nhanes)[1]))
print(paste0("N. observation by only using the determined predictors: ", dim(best_pred_nhanes)[1]))
```

```{r}
model_nhanes <- nhanes %>% 
				dplyr::select(AlcoholDay, Education, Depressed, Gender, Age, Poverty, SleepHrsNight, BPSysAve) %>%
				tidyr::drop_na()
```

## Data Wrangling

We'll have to do a few things before starting to train the model.
First, we have quite a few factors that needs on be on-hot encoded.

```{r}
dummies <- caret::dummyVars(~., data = model_nhanes)
onehot_nhanes <- predict(dummies, newdata=model_nhanes)
```

Then, our numerical values needs to be scaled.
Indeed, we'll implement a multi-layered perceptron architecture with a stochastic gradient descent backward function. As such, this function is not scale independant, and values of different scale influence the error propagation differently, thus inflating the importance of some features compared to other. More over, for the same reason, we'll also scale the target variable, our `y` and `AlcoholDay` in our case.

Additionnaly, we'll also center numerical values [-1, 1] to 
improve the initilization process and early convergence of the model. The choice between a [0, 1] or [-1, 1] normalization will really depends on activation function used. ReLu activation seems to benefit more from a [0, 1] range, as all negative values are 0, while sigmoid or tanh activation function seems to benefits from a normalization between the [-1, 1] range. According to the book `Hands-on Machine Learning with scikit-learn and TensorFlow by Aurélien Géron`, ReLu activation function is often a good default choice. Hence, we'll opt for a [0, 1] normalization range for now.

```{r}
cleaning = caret::preProcess(onehot_nhanes, method = c("range"), rangeBounds=c(0, 1))
prep_nhanes = predict(cleaning, newdata=onehot_nhanes)
```

## Train - Test split

It's important to split data to avoid overfitting and makesure that to have a reliable evaluation of the model. 
In the same vein, we'll also set up a cross-validation algorithm that we'll use when training.

Moreover, the prediction we'll make needs to generalize as good as possible to a large population. Hence, we'll also opt for leaving a portion of the training data out for the training. This help to regularize the model and avoid overfitting. However, our training sample size isn't very big to begin with, hence we'll leave out about 10% of the training data. This represent around 330 observations.

```{r}
train_idx <- createDataPartition(
																 y = prep_nhanes[,"AlcoholDay"],
																 p = 0.75,
																 list = FALSE
)

training <- prep_nhanes[train_idx,]
testing <- prep_nhanes[-train_idx,]
```

```{r}
cross_validation = trainControl(
																method = "repeatedcv",
																number = 5,
																repeats = 5,
																p = 0.1
)
```

# Model Fit

The model has already been decided as we'll implement a multilayered perceptrons. Now, some details needs to be ironed out, such as the number of hidden layers, their size, the activation functions. As a first impression, I'll drive conduct a first grid search to understand how to model interacts with the data better. 

As we have a small amount of data, we don't want a too large net of units, as that would make the model overfit quite easily. Why learn patterns when you can learn each observation by heart?

```{r}
print(paste0("Number of observations: ", dim(training)[1]))
print(paste0("Number of observations: ", dim(testing)[1]))
```

## Multi-layered Perceptron

### Training

The first simple approach is to use a Multi-layered perceptron from the `caret` package.
This implement don't allow for much customization as most of the model's parameters are hard-coded (activation function, loss-function, weight optimization algorithm).
As such, the loss function implemented depends on the model trained, but in our case it is the *Sum of Squared Error*.
Moreover, the weight optimization algorithm is the default gradient descent. 

Considering the result on the training set, it seems that the best version is the MLP with few amount of unit in the hidden layers.
The best result seems to be either 50 or 150.

```{r}
mlp_grid <- expand.grid(
				size = seq(from=50, to=1000, by=50)
)

mlp <- train(AlcoholDay ~.,
			data = training,
			method = "mlp",
			trControl = cross_validation,
			tuneGrid = mlp_grid,
)

print("Performance on the training phase.")
result = read.csv("neural_network/mlp_grid_result.csv")
result
```

```{r}
p <- ggplot(
			 data = result,
			 mapping = aes(x = size)) +
geom_line(mapping = aes(y = RMSE, color = "RMSE")) +
geom_line(mapping = aes(y = MAE, color="MAE")) +
scale_color_manual(values = c("MAE" = "#a8ddb5", "RMSE" = "#fdbb84")) +
labs(x = "n. hidden units", y = "performance", title="Model RMSE developement", color = "Perf. metric")

ggplotly(p)
```

### Testing

The testing phase delivers more insight into the performance of the model.
As we can see, it performs very poorly.
Moreover, as demonstrated by its inability to predict anything else that a single number, we can conclude that the model is too simple. 

```{r}
mlp_hu50 <- train(AlcoholDay ~.,
			data = training,
			method = "mlp",
			trControl = cross_validation,
			tuneGrid = expand.grid(size = c(50)),
)

mlp_hu150 <- train(AlcoholDay ~.,
			data = training,
			method = "mlp",
			trControl = cross_validation,
			tuneGrid = expand.grid(size = c(50)),
)

test_sample_idx = seq(dim(testing)[1] * 0.25)
y_pred_hu50 <- predict(mlp_hu50, newdata = testing[test_sample_idx,])
y_pred_hu150 <- predict(mlp_hu150, newdata = testing[test_sample_idx,])
```

```{r}
test_result = cbind(testing[test_sample_idx,], y_pred_hu50, y_pred_hu150)
ggplot(
			 data = test_result,
			 mapping = aes(x = seq(nrow(test_result)))) +
geom_line(mapping = aes(y = AlcoholDay, color = "truth")) +
geom_line(mapping = aes(y = y_pred_hu50, color = "hu50")) +
geom_line(mapping = aes(y = y_pred_hu150, color = "hu150")) +
scale_color_manual(values = c("hu50" = "#a8ddb5", "hu150" = "#fdbb84", "truth" = "#2ca25f")) +
labs(x = "observation", y = "daily OH consumption", title = "Model prediction comparison", color="model")
```

## Sequential Neural Network

A simple MLP architecture with a single layer lead to very shallow result.
In this section, I'll try an architecture slightly more complex, stacking a few hidden layers instead of a single. 

First let's create a validation set and proper X and y sets.

```{r}
val_idx <- createDataPartition(
																 y = training[, "AlcoholDay"],
																 p = 0.25,
																 list = FALSE
)

training <- prep_nhanes[train_idx,]
testing <- prep_nhanes[-train_idx,]
validation <- training[val_idx,]
training <- training[-val_idx,]

X_train <- training[, -c(1)]
y_train <- training[, 1]
X_val <- validation[, -c(1)]
y_val <- validation[, 1]
X_test <- testing[, -c(1)]
y_test <- testing[, 1]
```

Now, let's build the model architecture.
The `torch` library requires to use its own framework for dataset.


```{r}
nhanes_dataset <- dataset(
				name = "nhanes_dataset()",
				initialize = function(df) {
								df <- na.omit(df)
								self$x <- as.matrix(df[,-c(1)]) %>% torch_tensor()
								self$y <- as.matrix(df[, c(1)], ncol=1) %>% torch_tensor()
				},
				.getitem = function(i) {
								list(x = self$x[i,], y = self$y[i])
				},
				.length = function() {
								dim(self$x)[1]
				},
)

train_dataset <- nhanes_dataset(training)
valid_dataset <- nhanes_dataset(validation)
test_dataset <- nhanes_dataset(testing)

train_loader <- dataloader(train_dataset, batch_size=32, shuffle=TRUE)
valid_loader <- dataloader(valid_dataset, batch_size=32, shuffle=TRUE)
test_loader <- dataloader(test_dataset, batch_size=32, shuffle=TRUE)
```

Now, we define the architecture.
We'll have a netword with 3 hidden layers of 150 units each.
As for the activation function, we'll use the ReLu activation function for now, as it is a good default.
We'll also use the ADAM optimization function instead of thecommon gradient descent, as it is a more performant. 

```{r}
seqnn <- nn_module(
				initialize = function(d_in, d_hidden, d_out) {
								self$net <- nn_sequential(
												nn_linear(d_in, d_hidden),
												nn_relu(),
												nn_linear(d_hidden, d_hidden),
												nn_relu(),
												nn_linear(d_hidden, d_hidden),
												nn_relu(),
												nn_linear(d_hidden, d_hidden),
												nn_relu(),
												nn_linear(d_hidden, d_out)
								)
				},
				forward = function(x) {
								self$net(x)
				}
)
```
In the code below, we fit the previously created architecture using the `luz` library, which offer nice one-liners to compile results.

```{r}
fitted <- seqnn %>%
				setup(loss = nn_mse_loss(),
							optimizer = optim_adam,
							metrics = list(luz_metric_mae())) %>%
				set_hparams(
										d_in = 14,
										d_hidden = 150,
										d_out = 1) %>%
				fit(
						train_loader,
						epochs = 200,
						valid_data = valid_loader)
```

The result on the validation are fine.
The error at an average of 3 daily drinks remains stable throughout the whole range of target label values. 
A good point is the model generalize well as error rate don't drop between validation set testing set.
Quick remainders, the data has been transformed, hence `y_true` are not count anymore, although as we can see on the plot, they keep their discrete nature.
Hence, we reverse the `min-max` normalization procedure to have interpretable values.

```{r}
print("Validation set")
print(evaluate(fitted, valid_loader))
print("Testing set")
print(evaluate(fitted, test_loader))
```

```{r}
reverse_minmax <- function(y_true, max, min) {
				unnorm <- y_true * (max - min) + min
				return(unnorm)	
}

y_pred = predict(fitted, valid_loader) %>% as_array()
y_pred_test = predict(fitted, test_loader) %>% as_array()
y_true = valid_dataset$y %>% as_array()
y_pred_unnorm = reverse_minmax(y_pred, max(model_nhanes$AlcoholDay), min(model_nhanes$AlcoholDay))
y_pred_test_unnorm = reverse_minmax(y_pred_test, max(model_nhanes$AlcoholDay), min(model_nhanes$AlcoholDay))
y_true_unnorm <- reverse_minmax(y_true, max(model_nhanes$AlcoholDay), min(model_nhanes$AlcoholDay))

val_result <- data.frame(
										 y_true = y_true,
										 y_true_unnorm = y_true_unnorm,
										 y_pred = y_pred,
										 y_pred_unnorm = y_pred_unnorm
										 ) %>%				
				group_by(y_true_unnorm) %>%
				mutate(
							 y_pred_agg = mean(y_pred),
							 y_pred_unnorm_agg = mean(y_pred_unnorm),
				)

y_true = test_dataset$y %>% as_array()
y_true_unnorm <- reverse_minmax(y_true, max(model_nhanes$AlcoholDay), min(model_nhanes$AlcoholDay))
test_result <- data.frame(
										 y_true = y_true,
										 y_true_unnorm = y_true_unnorm,
										 y_pred_test = y_pred_test,
										 y_pred_test_unnorm = y_pred_test_unnorm
										 ) %>%				
				group_by(y_true_unnorm) %>%
				mutate(
							 y_pred_agg = mean(y_pred_test),
							 y_pred_unnorm_agg = mean(y_pred_test_unnorm),
				)
```

```{r}
val_plot <- ggplot(
			 data = val_result,
			 mapping = aes(x = y_true_unnorm)) +
geom_point(mapping = aes(y = y_pred_unnorm), color = "#a8ddb5") +
geom_line(mapping = aes(y = y_pred_unnorm_agg), color = "#fdbb84") +
labs(title="Residuals plot - validation set", x = "Groundtruth", y = "Residuals")
ggplotly(val_plot)

test_plot <- ggplot(
			 data = test_result,
			 mapping = aes(x = y_true_unnorm)) +
geom_point(mapping = aes(y = y_pred_test_unnorm), color = "#a8ddb5") +
geom_line(mapping = aes(y = y_pred_unnorm_agg), color = "#fdbb84") +
labs(title="Residuals plot - testing set", x = "Groundtruth", y = "Residuals")

ggplotly(val_plot)
ggplotly(test_plot)
```



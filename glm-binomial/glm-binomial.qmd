---
title: "Binomial GLM and GAM"
format: 
  html:
    toc: true
    toc-depth: 4
  
---

```{r}
#| label: "libraries"
#| message: false

library(tidyverse)
library(NHANES)
library(car)
library(pROC)
library(mgcv)

```

```{r}
#| label: "nhanes dataset"
#| include: false

data(NHANES)
# str(NHANES)

# Subset with ~20 variables (all types included)
nhanes_sub <- NHANES %>%
  select(
    # demography
    SurveyYr, Gender, Age, AgeDecade, Race1, Education, MaritalStatus,
    HHIncome, HHIncomeMid, Poverty, HomeRooms, HomeOwn,
    
    # health (continuous + categorical)
    Height, Weight, BMI, HealthGen,
    Pulse, BPSysAve, BPDiaAve,
    
    # behaviour & life style
    PhysActive, PhysActiveDays, SleepHrsNight, SleepTrouble,
    Alcohol12PlusYr, AlcoholDay, AlcoholYear,
    SmokeNow, Smoke100, SmokeAge,
    
    # mental health
    DaysPhysHlthBad, DaysMentHlthBad,
    LittleInterest, Depressed
  )

str(nhanes_sub)

```

## Binomial models

### Make outcome variable Depressed (three levels) binary

```{r}
#| label: "Depressed binomial"

summary(nhanes_sub$Depressed)

nhanes_sub <- nhanes_sub %>%
  mutate(
    Depressed_bin = case_when(
      Depressed == "None" ~ "no",
      Depressed %in% c("Several", "Most") ~ "yes",
      TRUE ~ NA_character_
    ),
    Depressed_bin = factor(Depressed_bin, levels = c("no", "yes"))
  )

# Check the recode
table(nhanes_sub$Depressed, nhanes_sub$Depressed_bin, useNA = "ifany")

```

### Visual inspection of Variable Depressed_bin

```{r}
#| label: "Depressed_bin Visuals"

# Age vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = Depressed_bin, y = Age)) +
  geom_boxplot()

# SleepHrsNight vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = Depressed_bin, y = SleepHrsNight)) +
  geom_boxplot()

# Gender vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = Gender, fill = Depressed_bin)) +
  geom_bar(position = "fill")

# PhysActive vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = PhysActive, fill = Depressed_bin)) +
  geom_bar(position = "fill")

# MaritalStatus vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = MaritalStatus, fill = Depressed_bin)) +
  geom_bar(position = "fill") +
  theme(axis.text.x = element_text(angle=45, hjust=1))

# PhysActive nach MaritalStatus
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), 
       aes(x = PhysActive, fill = Depressed_bin)) +
  geom_bar(position = "fill") +
  facet_wrap(~ MaritalStatus)

# BMI vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = Depressed_bin, y = BMI)) +
  geom_boxplot()

# Poverty vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = Depressed_bin, y = Poverty)) +
  geom_boxplot()

# HealthGen vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = HealthGen, fill = Depressed_bin)) +
  geom_bar(position = "fill")

# SleepTrouble vs Depressed
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), aes(x = SleepTrouble, fill = Depressed_bin)) +
  geom_bar(position = "fill")

# library(ggmosaic)
# ggplot(filter(nhanes_sub, !is.na(Depressed_bin))) +
#  geom_mosaic(aes(x = product(Gender, MaritalStatus), fill = Depressed_bin)) +
#  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# SleepTrouble nach Gender
ggplot(filter(nhanes_sub, !is.na(Depressed_bin)), 
       aes(x = SleepTrouble, fill = Depressed_bin)) +
  geom_bar(position = "fill") +
  facet_wrap(~ Gender)

```

### Generalized Linear Model (family: Binomial)

#### First model (selection of variables)

Previous research has shown that depression is often associated with demographic, socioeconomic, social, health, and lifestyle factors. Based on these findings and after visual inspection of the graphs above, the following 9 variables are included in the first model:

-   **Demographic**: Age, Gender\
-   **Socioeconomic**: Poverty\
-   **Social**: MaritalStatus\
-   **Health-related**: BMI, HealthGen\
-   **Sleep / Activity**: SleepHrsNight, SleepTrouble, PhysActive

These choices are supported by findings in NHANES-based studies, where factors such as poverty, marital status, physical activity, sleep problems, BMI, and general health perception were among the strongest predictors of depression (see e.g. Zhang et al., *BMC Medical Informatics and Decision Making*, 2025).

```{r}
#| label: "GLM_Binomial_1"

Depressed_data <- nhanes_sub %>%
  filter(!is.na(Depressed_bin))

glm_depressed_1 <- glm(Depressed_bin ~ Age + Gender + Poverty + MaritalStatus + BMI +
                   HealthGen + SleepHrsNight + SleepTrouble + PhysActive,
                   data = Depressed_data,
                   family = binomial)
summary(glm_depressed_1)


```

#### Handling of missings

703 observation were removed as **missing**. In which variable are they missing?

```{r}
#| label: "GLM_Binomial_missings"

vars_model <- c("Depressed_bin", "Age", "Gender", "Poverty", "MaritalStatus", "BMI", "HealthGen", "SleepHrsNight", "SleepTrouble", "PhysActive")

Depressed_data %>%
  select(all_of(vars_model)) %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(),
                      names_to = "Variable", values_to = "Missing_Count")



```

It is feasible to **replace missings** in `Poverty`, `BMI` and `SleepHrsNight` by the **median**. For `MaritalStatus` I don't see a convincing approach to replace missings.

```{r}
#| label: "Impute_Missings"

Depressed_data_imp <- Depressed_data %>%
  mutate(
    Poverty = ifelse(is.na(Poverty), median(Poverty, na.rm = TRUE), Poverty),
    BMI = ifelse(is.na(BMI), median(BMI, na.rm = TRUE), BMI),
    SleepHrsNight = ifelse(is.na(SleepHrsNight), median(SleepHrsNight, na.rm = TRUE), SleepHrsNight),)

Depressed_data_imp %>%
  select(all_of(vars_model)) %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(),
                      names_to = "Variable", values_to = "Missing_Count")

```

I will continue the modelling with this dataset where only `MaritalStatus` is missing for 236 cases.

```{r}
#| label: "GLM_Binomial_1b"

glm_depressed_1b <- glm(Depressed_bin ~ Age + Gender + Poverty + MaritalStatus + BMI +
                   HealthGen + SleepHrsNight + SleepTrouble + PhysActive,
                   data = Depressed_data_imp,
                   family = binomial)
summary(glm_depressed_1b)

```

#### First interpretation

There is **strong evidence against the null hypothesis** that `Gender`, `Poverty`, `MaritalStatus`, `HealthGen`, `SleepHrsNight`, `SleepTrouble` and `PhysActivitie` have no effect. Whereas `Age` and `BMI` show **no significant effect** on Depressed_bin.

#### Checking for Collinearity

I am concerned about potential collinearity between the predictors, as this could hinder a proper interpretation of their individual effects. Therefore, I checked the model for signs of collinearity.

```{r}
#| label: "GLM_Binomial_1b_Collinear"

vif(glm_depressed_1b)   # auf dein Modell anwenden


```

All VIF values are \<2, meaning the model does not suffer from collinearity.

#### Second model (Age, BMI removed)

```{r}
#| label: "GLM_Binomial_2"

glm_depressed_2 <- glm(Depressed_bin ~ Gender + Poverty + MaritalStatus + HealthGen + SleepHrsNight + SleepTrouble + PhysActive,
                       data = Depressed_data_imp,
                       family = binomial)
summary(glm_depressed_2)

```

Do all variables **contribute significantly** to the quality of the model?

```{r}
#| label: "Glm_depressed_2_drop1"

drop1(glm_depressed_2, test = "Chisq")

```

As expected, all 7 variables are highly relevant for the quality of the model

#### Third model: Conceptual collinearity

I think that the variable **`HealthGen`** is **conceptually collinear** with `Depressed_bin`, as self-rated general health strongly overlaps with depressive symptoms. Since the focus of this analysis is on understanding underlying relationships rather than maximizing predictive accuracy, **`HealthGen`** will be excluded from the final model.

```{r}
#| label: "GLM_Binomial_3"

glm_depressed_3 <- glm(Depressed_bin ~ Gender + Poverty + MaritalStatus + SleepHrsNight + SleepTrouble + PhysActive,
                       data = Depressed_data_imp,
                       family = binomial)
summary(glm_depressed_3)

```

#### Comparison 2nd and 3rd model

The 2nd model has Residual deviance of 5948.8, the 3rd of 6095.2. Therefore **both differ clearly from the Null deviance** of 6688.1. The 2nd model including `HealthGen` is slightly better, also while comparing AIC (5978.8 vs. 6117.2). However, **I prefer the third model**, as `HealthGen` is not causally independent from `Depressed_bin` and may reflect overlapping constructs rather than a distinct explanatory factor.

To evaluate the discriminatory power of the logistic regression model (3rd), a **ROC (Receiver Operating Characteristic)** analysis is performed using the pROC package.

```{r}
#| label: "GLM_Binomial_3_ROC"
#| message: false

roc(glm_depressed_3$y, fitted(glm_depressed_3))

# ROC-Kurve plotten
roc_obj <- roc(glm_depressed_3$y, fitted(glm_depressed_3))
plot(roc_obj)

# Find optimal threshold (e.g. Youden-Index)
coords(roc_obj, "best", ret = "threshold")

```

The resulting **AUC of 0.71** indicates that the model distinguishes **reasonably well** between depressed and non-depressed individuals. While not exceptionally high, this level of accuracy is typical for psycho-social data and supports the model’s usefulness for explanatory purposes.
We come back to the optimal threshold at the very end of this chapter when having a closer look at the confusion matrices.

### Generalized Additive Model (family: Binomial)

Taking non-linear effects into account could improve the model. Therefore I check first visually if there might be non-linear relations between `Depressed_bin` and the variables `Poverty` and `SleepHrsNight`.

#### Exploratory plots

```{r}
#| label: "Plot_nonlinear_effects"
#| message: false
#| warning: false

ggplot(Depressed_data_imp, aes(x = Poverty, y = as.numeric(Depressed_bin) - 1)) +
  geom_jitter(height = 0.05, alpha = 0.3, color = "gray60") +
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              color = "blue", se = FALSE) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(
    y = "Probability of Depression",
    title = "Poverty vs. Probability of Depression",
    subtitle = "Blue = logistic fit (GLM), Red = flexible smoother (LOESS)"
  )

ggplot(Depressed_data_imp, aes(x = SleepHrsNight, y = as.numeric(Depressed_bin) - 1)) +
  geom_jitter(height = 0.05, alpha = 0.3, color = "gray60") +
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              color = "blue", se = FALSE) +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  labs(
    y = "Probability of Depression",
    title = "Sleep hours per night vs. Probability of Depression",
    subtitle = "Blue = logistic fit (GLM), Red = flexible smoother (LOESS)"
  )


```

The exploratory plots suggest that the relationship between `Poverty` and depression is not strictly linear, while `SleepHrsNight` shows a clear non-linear, U-shaped association with depression probability.

#### Adding smooth splines to the model

To capture these **non-linear patterns** more accurately, a Generalized Additive Model (**GAM**) with **smooth spline terms** for `Poverty` and `SleepHrsNight` is being estimated using the `mgcv` package.

```{r}
#| label: "GAM_Binomial_1"
#| message: false

gam_depressed_1 <- gam(Depressed_bin ~ s(Poverty) + s(SleepHrsNight) +
                       Gender + MaritalStatus + SleepTrouble + PhysActive,
                     data = Depressed_data_imp, family = binomial)
summary(gam_depressed_1)

```

The GAM results confirm that both `Poverty` and `SleepHrsNight` exhibit **significant non-linear effects** on depression probability, as indicated by high estimated degrees of freedom (**edf \> 6**) and p-values \< 0.001.

#### Final interpretation of the GAM model

We have strong evidence against the null hypothesis that several factors have no effect on depression. Being **male** (Estimate = –0.31, p \< 0.001), being **married** (–0.67, p \< 0.001), and being **physically active** (–0.38, p \< 0.001) are all associated with lower odds of depression, while experiencing **sleep trouble** (+0.85, p \< 0.001) is associated with substantially higher odds. Individuals who are **widowed** also show a significantly lower risk (–0.54, p \< 0.001) compared to the reference category.

The smooth terms for **Poverty** (edf = 6.29, p \< 0.001) and **SleepHrsNight** (edf = 7.84, p \< 0.001) further indicate **significant non-linear effects** on depression probability. Overall, the model explains about **10% of the deviance**, suggesting that both socioeconomic and lifestyle factors meaningfully contribute to the likelihood of depression.

### Measure of Fit

#### ANOVA: comparing GLM and GAM models

As a **preliminary check**, an analysis of deviance (ANOVA) is being conducted to test whether the GAM provides a statistically significant improvement over the simpler GLM.

```{r}
#| label: "ANOVA_GLM_GAM"
#| message: false

anova(glm_depressed_3, gam_depressed_1, test = "Chisq")

```

The analysis of deviance confirmed that **the GAM significantly improved model fit** compared to the linear model (χ² = 69.10, df = 12.14, p \< 0.001), indicating that the inclusion of non-linear smooth terms for `Poverty` and `SleepHrsNight` provides a statistically better representation of the data.

#### Akaike Information Criterion (AIC)

The AIC is a commonly used measure of model fit that balances model accuracy and complexity. It is chosen because it allows a direct **comparison between models with different numbers of parameters**, penalizing excessive complexity while rewarding better data fit.

```{r}
#| label: "AIC_GLM_GAM"
#| message: false

AIC(glm_depressed_3, gam_depressed_1)

```

The **GAM achieved a lower AIC** (6072.4) compared to the linear GLM (6117.2), indicating a **better overall model fit** even after accounting for the increased model complexity.

### Confusion matrix

Now we directly check how good the prediction of the model are. Therefore we assign predictions \> 0.5 as 1 and \< 0.5 as 0. First we define a function providing accuracy, sensitivity, specifity and precision.

```{r}
#| label: "Confusion_Matrix_GLM"

# Function for confusion matrix
eval_model <- function(model, threshold = 0.5) {
  # predicted probabilities
  pred_prob <- fitted(model)
  
  # classification based on predicted probabilities
  pred_class <- ifelse(pred_prob > threshold, "yes", "no")
  pred_class <- factor(pred_class, levels = c("no", "yes"))
  
  # real values
  actual <- model$y
  actual <- factor(ifelse(actual == 1, "yes", "no"), levels = c("no", "yes"))
  
  # Confusion Matrix
  cm <- table(Predicted = pred_class, Actual = actual)
  
  # calculate the measures
  accuracy <- sum(diag(cm)) / sum(cm)
  sensitivity <- cm[2,2] / sum(cm[,2])  # True Positive Rate ('Recall')
  specificity <- cm[1,1] / sum(cm[,1])  # True Negative Rate
  precision <- cm[2,2] / sum(cm[2,])    # Positive Predictive Value
  
  # Output
  list(
    confusion_matrix = cm,
    accuracy = accuracy,
    sensitivity = sensitivity,
    specificity = specificity,
    precision = precision
  )
}

```

Then we use the function to calculate these measures for the two models.

```{r}
#| label: "Evaluate models"

# Evaluate GLM Model 3
cat("=== GLM Model 3 (without Age, BMI, HealthGen) ===\n")
eval_3 <- eval_model(glm_depressed_3)
print(eval_3$confusion_matrix)
cat("\nAccuracy:", round(eval_3$accuracy, 3))
cat("\nSensitivity (Recall):", round(eval_3$sensitivity, 3))
cat("\nSpecificity:", round(eval_3$specificity, 3))
cat("\nPrecision:", round(eval_3$precision, 3))

# Evaluate GAM Model 1
cat("\n\n=== GAM Model 1 (with Age and BMI) ===\n")
eval_1 <- eval_model(gam_depressed_1)
print(eval_1$confusion_matrix)
cat("\nAccuracy:", round(eval_1$accuracy, 3))
cat("\nSensitivity (Recall):", round(eval_1$sensitivity, 3))
cat("\nSpecificity:", round(eval_1$specificity, 3))
cat("\nPrecision:", round(eval_1$precision, 3))

```

#### Interpretation: Confusion Matrix Results (GLM vs GAM)

Both models show **high accuracy** (\~79%), but this is misleading due to the **imbalanced dataset** (most observations are non-depressed).

**Key problem**: Both models are **extremely conservative** at the default 0.5 threshold: - Very low **sensitivity** (9-12%) – they miss most depressed cases - Very high **specificity** (\~97%) – they rarely misclassify non-depressed individuals - Moderate **precision** (50-54%)

The **GAM performs slightly better** than the GLM, with higher sensitivity (11.8% vs. 9.4%) and precision (54% vs. 50%), suggesting that modeling non-linear effects provides marginal improvement in identifying depressed individuals.

We could **lower the classification threshold** (e.g., to 0.3) to improve sensitivity, accepting a trade-off with lower specificity. This is more appropriate for screening purposes.

```{r}
#| label: "Evaluate models with lower threshold"

# Evaluate GLM Model 3
cat("=== GLM Model 3 (threshold = 0.3) ===\n")
eval_3 <- eval_model(glm_depressed_3, threshold = 0.3)
print(eval_3$confusion_matrix)
cat("\nAccuracy:", round(eval_3$accuracy, 3))
cat("\nSensitivity (Recall):", round(eval_3$sensitivity, 3))
cat("\nSpecificity:", round(eval_3$specificity, 3))
cat("\nPrecision:", round(eval_3$precision, 3))

# Evaluate GAM Model 1
cat("\n\n=== GAM Model 1 (threshold = 0.3) ===\n")
eval_1 <- eval_model(gam_depressed_1, threshold = 0.3)
print(eval_1$confusion_matrix)
cat("\nAccuracy:", round(eval_1$accuracy, 3))
cat("\nSensitivity (Recall):", round(eval_1$sensitivity, 3))
cat("\nSpecificity:", round(eval_1$specificity, 3))
cat("\nPrecision:", round(eval_1$precision, 3))

```

#### Interpretation: Threshold = 0.3

Lowering the threshold to 0.3 substantially **improves detection of depressed individuals**: The GAM now identifies **43.5%** of depressed cases (vs. 11.8% at 0.5), while the GLM identifies **41.8%** (vs. 9.4%).

The trade-off is **lower specificity** (~84%), meaning more false positives. However, this is often acceptable in mental health screening, where missing a depressed person is more problematic than a false alarm.

**Precision** remains around **42%** – when depression is predicted, it's correct less than half the time. While this seems low, it's reasonable for an initial screening tool that would be followed by clinical assessment.

**Conclusion**: With threshold = 0.3, both models are more balanced. The **GAM performs slightly better**, with higher sensitivity (43.5% vs. 41.8%), supporting the inclusion of non-linear effects for `Poverty` and `SleepHrsNight`.


### Note on Overdispersion

Since the outcome variable `Depressed_bin` is **binary** (individual observations coded as 0/1), there is **no concern about overdispersion** in this analysis. Overdispersion is only relevant for **aggregated binomial data** (e.g., "15 successes out of 20 trials"), where the observed variance can exceed the variance assumed by the binomial model.

In this case, each observation follows a **Bernoulli distribution** (a special case of the binomial distribution with n=1), where the variance is fixed at p(1-p) and cannot be overdispersed.
